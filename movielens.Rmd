---
title: "Movie Lens Project"
author: "Allison Patch"
date: "`r format(Sys.Date())`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In October 2006, Netflix ran a contest to create a better movie recommendation system for its users. This project uses a portion of the data from Netflix's contest to attempt to create a recommendation algorithm that has a root mean squared error (RMSE) of 0.87750 or below. RMSE is important because it is a measurement of the average error our algorithm produces, which is helpful because it gives us an idea of how reliable our algorithm is. 

The process of algorithm selection I used in my analysis was Matrix factorization. I chose this method because it allowed me to evaluate the success of iterative models with new parameter vectors included in each model iteration. This allows for easy comparison between models and shows how each new parameter influences the RMSE. 

The following sections of this report are divided as follows: Methods, Results, and Conclusion. The Methods section will go over the data cleaning and exploration process and then set up the algorithm-building portion of the project. The results section will detail the results of the algortihm exploration. Finally, the conclusion will bring together the bigger picture of the results, and discuss other factors that could help in future research on this topic.

## Methods

The first step to the process was downloading the data and splitting it into a training (data name: edx) and test set (data name: validation). For my project I used the code provided through the EdX course. Following this, I conducted a bit of data exploration guided by the quiz given in the EdX course. Finally, I developed several iterative models using the training set and measuring the quality of the model by calculating the RMSE using the test set for each model. The first two sections of code can be found together below. The remainder of the report will discuss the results of my recommendation system algorithm exploration.

```{r Data, message=FALSE}
######################################################################################
# Create edx set, validation set, and submission file#################################
######################################################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)


#################################################################################################
## Exploring Data ##############################################################################
#################################################################################################

#getting row and column length
nrow(edx)
ncol(edx)


#frequency table to ratings to get number of 0 and 3 in ratings
table(edx$rating)

#number of unique movies
length(unique(edx$movieId))

#number of unique users
length(unique(edx$userId))

edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))

#Create data which give each genre category for a movie a separate entry 
#Same movie_rating will occur in multiple rows if movie is categorized into multiple genres
by_genre<-separate_rows(edx, genres, convert = TRUE)

#number of movies by genre
by_genre %>% group_by(genres) %>% summarize(count=length(rating)) 
##Do this again for validation dataset for future testing
by_genre_v<-separate_rows(validation, genres, convert = TRUE)


#Movie with highest rating
edx%>% group_by(title)%>% summarize(count=length(rating)) %>% arrange(desc(count))

```

### Methods--Algorithm Exploration

In thinking of how to create the most accurate movie recommendation systems, we first have to reflect on the factors that go into movie recommendations. First, there is a general average rating that movies have. Second, we can think of how movies compare to the average---are they better, worse, the same? Third, we need to account for the individual movie watchers because every person has personal preferences and those preferences are likely to be seen through the individual's rating patterns. Finally, we need to think about the types of movies that are being shown. The different genres movies fall into are also likely to influence the rating we can expect a movie to receive. 

## Results
The following sections of this report exmaine each of these factors in turn, building to the final algortihm for recommendation that I produce. 

To begin, I created a fuction which would calculate the RMSE for each of the models.
```{r RMSE calc}
###Since the outcome of interest is RMSE, create a function to calculate RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The overall average movie rating is the staring point to the analysis for my algorithms. The overall average was:

```{r mu, echo=FALSE}
###First, look at just the average movie rating and use Naive Bayes
##mu_hat is the overall average rating
mu_hat <- mean(edx$rating)
```

Using this overall average, the first model I ran used a naive bayes approach simply looking at the overall average ratings given and how well that could predict what rating movies in the test set woul be rated.
``` {r nb model}

###calculate RMSE for the overall average
overall_average_rmse <- RMSE(validation$rating, mu_hat)
overall_average_rmse

###Here I am creating a tibble to hold the results of the models to compare RMSE
rmse_results <- tibble(method = "Overall average", RMSE = overall_average_rmse)
```

``` {r table1, echo=FALSE, fig.width=3}
#View the table
rmse_results %>% knitr::kable()

```

The RMSE result for the Overall Average Model is over 1, which is not very good. To improve this we will add another parameter to the model: Movie Effects. This will account for the difference in each movie's average rating from the overall average. 

```{r movie averages}
## b_hat_i is the average of the diffence between the rating given to the movie minus the overall average rating
mu_hat <- mean(edx$rating) 
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_hat_i = mean(rating - mu_hat))
```


A histogram of this difference shows a mostly normal distribution around 0.

``` {r avg movie id diff, echo=FALSE}
##Here we visualize the distribution of b_i
movie_avgs %>% qplot(b_hat_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

I then ran a second model including Movie Effects
``` {r movie effects}
##Now we use the validation data to check the RSME for the mode

#first we have to calculate b_hat_i for the validation set and calculate the predicted ratings for the validation #model
predicted_ratings <- mu_hat + validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_hat_i

#next we test to see the RMSE 
model_movieId_rmse <- RMSE(predicted_ratings, validation$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_movieId_rmse ))
```

``` {r movie results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

The RMSE result for the Movie Effects is still not very low so I added another parameter to the model: User Effects.A histogram of this difference shows a mostly normal distribution around 3.75.

```{r mean user, echo=FALSE}
## b_hat_u is the average rating given by each user
edx %>% 
  group_by(userId) %>% 
  summarize(b_hat_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_hat_u)) + 
  geom_histogram(bins = 30, color = "black")
```
I then ran a third model including Movie Effects and User Effects
```{r movie+user}
##Now we use the validation data to check the RSME for the mode
#first we have to create user averages for the validation set so that we can make our predictions
user_avgs <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_hat_u = mean(rating - mu_hat - b_hat_i))

## Then we have to calculate the predicted ratings for the validation mode
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu_hat + b_hat_i + b_hat_u) %>%
  .$pred

#next we test to see the RMSE 
model_movieUser_rmse <- RMSE(predicted_ratings, validation$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_movieUser_rmse ))
```
```{r m+e results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

We see that this RMSE result is sufficiently low for our recommendation system to qualify as a success

The RMSE result is fine, but we want to see if there is any additional help from: Genre Effects.A histogram of this difference shows a mostly normal distribution around 3.5.

```{r genre, echo=FALSE}
## b_hat_g_g is the average rating given for each genre category
edx %>% 
  group_by(genres) %>% 
  summarize(b_hat_g = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_hat_g)) + 
  geom_histogram(bins = 30, color = "black")
```
I then ran a fourth model including Movie Effects, User Effects, and Genre Effects.
```{r m+e+g}
##Now we use the validation data to check the RSME for the mode
#first we have to create user averages for the validation set so that we can make our predictions
genre_avgs <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_hat_g = mean(rating - mu_hat - b_hat_i - b_hat_u))

## Then we have to calculate the predicted ratings for the validation mode
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu_hat + b_hat_i + b_hat_u +b_hat_g) %>%
  .$pred

#next we test to see the RMSE 
model_movieUserGenre_rmse <- RMSE(predicted_ratings, validation$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User + Genre Effects Model",  
                                     RMSE = model_movieUserGenre_rmse ))
```

```{r m+e+g results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

However, in exploring the data, I discovered that individual movies were grouped into several genre categories at once, which made it difficult to really examine the genre effect. To account for this, I re-created the datasets with each genre for a movie receiving its own row. I then re-ran all of the algorithms I had originally created on these new datasets.

```{r with genre split, include=FALSE}
## mu_hat_g is the overall average rating
mu_hat_g <- mean(by_genre$rating)
mu_hat_g


### calculate RMSE for the overall average
overall_average_rmse_g <- RMSE(by_genre_v$rating, mu_hat_g)
overall_average_rmse_g

### Next we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method = "Split Genres: Overall average", 
                                     RMSE = overall_average_rmse_g))

#View the table
rmse_results %>% knitr::kable()

## Again, we see that the RMSE result for the Overall Average Model with the split genre data is over 1, which is not #very good

## However, we see that using the data split by genre helps to lower RMSE ever slightly more than using the original #data

## To improve this we will again add another parameter to the model: Movie Effects
## This will account for the difference in each movie's average rating from the overall average

## b_hat_i_g is the average of the diffence between the rating given to the movie minus the overall average rating

mu_hat_g <- mean(by_genre$rating) 
movie_avgs <- by_genre %>% 
  group_by(movieId) %>% 
  summarize(b_hat_i_g = mean(rating - mu_hat_g))


##Here we visualize the distribution of b_hat_i_g
movie_avgs %>% qplot(b_hat_i_g, geom ="histogram", bins = 10, data = ., color = I("black"))

##Now we use the validation data to check the RSME for the mode

# Again, first we have to calculate b_hat_i_g for the validation set and calculate the predicted ratings for the #validation model
predicted_ratings <- mu_hat_g + by_genre_v %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_hat_i_g

#next we test to see the RMSE 
model_movieId_rmse_g <- RMSE(predicted_ratings, by_genre_v$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Split Genres: Movie Effect Model",
                                     RMSE = model_movieId_rmse_g ))
#View the table
rmse_results %>% knitr::kable()

##The RMSE result for the Movie Effects on the data split by genre is still not very low so we will add another #parameter to the model: User Effects
## However, we see that using the data split by genre helps to lower RMSE ever slightly more than using the original #data

## b_hat_u_g is the average rating given by each user
by_genre %>% 
  group_by(userId) %>% 
  summarize(b_hat_u_g = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_hat_u_g)) + 
  geom_histogram(bins = 30, color = "black")

##Now we use the validation data to check the RSME for the mode

#first we have to create user averages for the validation set so that we can make our predictions
user_avgs <- by_genre_v %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_hat_u_g = mean(rating - mu_hat_g - b_hat_i_g))

## Then we have to calculate the predicted ratings for the validation mode
predicted_ratings <- by_genre_v %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu_hat_g + b_hat_i_g + b_hat_u_g) %>%
  .$pred

#next we test to see the RMSE 
model_movieUser_rmse_g <- RMSE(predicted_ratings, by_genre_v$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Split Genres: Movie + User Effects Model",  
                                     RMSE = model_movieUser_rmse_g ))

#View the table
rmse_results %>% knitr::kable()

## Again we see that this RMSE result for Movie and User Effects is sufficiently low for our recommendation system to #qualify as a success
## Additionally, we see that using the data split by genre helps to lower RMSE ever slightly more


##The RMSE result is fine, but we want to see if there is any additional help from: Genre Effects

## b_hat_g_g is the average rating given for each genre category

by_genre %>% 
  group_by(genres) %>% 
  summarize(b_hat_g_g = mean(rating)) %>% 
  filter(n()>=1) %>%
  ggplot(aes(b_hat_g_g)) + 
  geom_histogram(bins = 30, color = "black")

#Note we see that the distribution of this does not follow the normal distribution 

##Now we use the validation data to check the RSME for the mode

#first we have to create user averages for the validation set so that we can make our predictions
genre_avgs <- by_genre_v %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_hat_g_g = mean(rating - mu_hat_g - b_hat_i_g- b_hat_u_g))

## Then we have to calculate the predicted ratings for the validation mode
predicted_ratings <- by_genre_v %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu_hat_g + b_hat_i_g + b_hat_u_g +b_hat_g_g) %>%
  .$pred

#next we test to see the RMSE 
model_movieUserGenre_rmse_g <- RMSE(predicted_ratings, by_genre_v$rating)

#finally we add this model to our RSME table 
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Split Genres: Movie + User + Genre Effects Model",  
                                     RMSE = model_movieUserGenre_rmse_g ))
```
```{r with split results, echo=FALSE}
#View the table
rmse_results %>% knitr::kable()
```

With these new models, we see that using the data split by genre helps to lower RMSE ever slightly more. 


## Conclusion

In attempting to create a high quality recommendation system for Netflix users with the dataset provided, it appears that two parameters combine to provide a relatively good system: Movie Effects and User Effects. The RMSE for this model is about 0.829. Including genre effects also appears to improve the model slightly, these differences may be a bit too small to justify the potential for over-training the model. 

Knowing what movies a particular user likes and what the average ratings for particular movies are appears to be enough information to provide a decent recommendation for other movies that user would like. Several other demographic parameters (such as age, gender, education, etc.) that would help define the popluation individual users belong to could help to introduce other factors that could help direct the recommendation system to an even greater accuracy. 